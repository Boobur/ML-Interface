{
    "DecisionTreeRegressor":{
        "title":"Qaror daraxti regressori.",
        "params":[
            {
                "criterion":"{squared_error, friedman_mse, absolute_error, poisson}, default=squared_error",
                "description":"The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits."
               
            },
            {
                "splitter":"{best, random}, default=”best”",
                "description":"The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split."
               
            },
            {
                "max_depth": "int, default=None",
                "description":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
               
            },
            {
                "min_samples_split":"int or float, default=2",
                "description":"The minimum number of samples required to split an internal node:\n If int, then consider min_samples_split as the minimum number.\n If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split."
               
            },
            {
                "min_samples_leaf":"int or float, default=1",
                "description":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. \n If int, then consider min_samples_leaf as the minimum number. \n If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node."
               
            },
            {
                "min_weight_fraction_leaf": "float, default=0.0",
                "description":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided."
            },
            {
                "max_features" : "int, float or {“sqrt”, “log2”}, default=None",
                "description": "The number of features to consider when looking for the best split:\n If int, then consider max_features features at each split. \n If float, then max_features is a fraction and max(1, int(max_features * n_features_in_)) features are considered at each split. \n If “sqrt”, then max_features=sqrt(n_features).\n If “log2”, then max_features=log2(n_features).\n If None, then max_features=n_features."
            },
            {
                "min_impurity_decrease": "float, default=0.0",
                "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n The weighted impurity decrease equation is the following: \n\n N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)"
            },
            {
                "random_state":"int, RandomState instance or None, default=None",
                "description":"Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to best.When max_features <n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details."
            },
            {
                "ccp_alpha": "non-negative float, default=0.0",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details."
            },
            {
                "monotonic_cst":"array-like of int of shape (n_features), default=None",
                "description":"Indicates the monotonicity constraint to enforce on each feature.\n 1: monotonic increase \n0: no constraint \n-1: monotonic decrease"
            }

        ]
    },
    "RandomForestRegressor":{
        "title":"Tasodifiy o'rmon regressori.",
        "params":[
            {
                "n_estimators":"int, default=100",
                "description": "The number of trees in the forest."
            },
            {
                "criterion":"{squared_error, friedman_mse, absolute_error, poisson}, default=squared_error",
                "description":"The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits."
                
            },
            {
                "max_depth": "int, default=None",
                "description":"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
                
            },
            {
                "min_samples_split":"int or float, default=2",
                "description":"The minimum number of samples required to split an internal node:\n If int, then consider min_samples_split as the minimum number.\n If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split."
                
            },
            {
                "min_samples_leaf":"int or float, default=1",
                "description":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. \n If int, then consider min_samples_leaf as the minimum number. \n If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node."
                
            },
            {
                "min_weight_fraction_leaf": "float, default=0.0",
                "description":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided."
            },
            {
                "max_features" : "int, float or {“sqrt”, “log2”}, default=sqrt",
                "description": "The number of features to consider when looking for the best split:\n If int, then consider max_features features at each split. \n If float, then max_features is a fraction and max(1, int(max_features * n_features_in_)) features are considered at each split. \n If “sqrt”, then max_features=sqrt(n_features).\n If “log2”, then max_features=log2(n_features).\n If None, then max_features=n_features."
            },
            {
                "max_leaf_nodes":"int, default=None",
                "description":"Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes."
            },
            {
                "min_impurity_decrease": "float, default=0.0",
                "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n The weighted impurity decrease equation is the following: \n\n N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)"
            },
            {
                "bootstrap":"bool, default=True",
                 "description":"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree."

            },
            {
                "oob_score":"bool or callable, default=False",
                "description":"Whether to use out-of-bag samples to estimate the generalization score. By default, accuracy_score is used. Provide a callable with signature metric(y_true, y_pred) to use a custom metric. Only available if bootstrap=True.n_jobsint, defa"
            },
            {
                "random_state":"int, RandomState instance or None, default=None",
                "description":"Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to best.When max_features <n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. See Glossary for details."
            },
            {
                "verbose":"int, default=0",
                "description":"Controls the verbosity when fitting and predicting."
            },
            {
                "warm_start":"bool, default=False",
                "description":"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See Glossary and Fitting additional weak-learners for details."
            },
            {
                "class_weight":"{“balanced”, “balanced_subsample”}, dict or list of dicts, default=None",
                "description":"Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. \n Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. \n The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n The “balanced_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown. \n For multi-output, the weights of each column of y will be multiplied. \n Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified."
            },
            {
                "ccp_alpha": "non-negative float, default=0.0",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details."
            },
            {
                "max_samples":"int or float, default=None",
                "description":"If bootstrap is True, the number of samples to draw from X to train each base estimator.\n If None (default), then draw X.shape[0] samples.\n If int, then draw max_samples samples. \n If float, then draw max(round(n_samples * max_samples), 1) samples. Thus, max_samples should be in the interval (0.0, 1.0]."
            },
            {
                "monotonic_cst":"array-like of int of shape (n_features), default=None",
                "description":"Indicates the monotonicity constraint to enforce on each feature.\n 1: monotonic increase \n0: no constraint \n-1: monotonic decrease"
            }

        ]
    },
    "GradientBoostingRegressor":{
        "title":"Gradient Boosting regressori.",
        "params":[
            {
                "loss":"{squared_error, absolute_error, huber, quantile}, default=squared_error",
                "description": "Loss function to be optimized. ‘squared_error’ refers to the squared error for regression. ‘absolute_error’ refers to the absolute error of regression and is a robust loss function. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile)."
            },
            {
                "learning_rate":"float, default=0.1",
                "description":"Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators. Values must be in the range [0.0, inf)."
                
            },
            {
                "n_estimators": "int, default=100",
                "description":"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. Values must be in the range (0.0, 1.0]."
                
            },
            {
                "criterion":"{‘friedman_mse’, ‘squared_error’}, default=’friedman_mse’",
                "description":"The function to measure the quality of a split. Supported criteria are “friedman_mse” for the mean squared error with improvement score by Friedman, “squared_error” for mean squared error. The default value of “friedman_mse” is generally the best as it can provide a better approximation in some cases."
            },
            {
                "min_samples_split":"int or float, default=2",
                "description":"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n IIf int, values must be in the range [1, inf)\n IIf float, values must be in the range (0.0, 1.0) and min_samples_leaf will be ceil(min_samples_leaf * n_samples)."
                
            },
            {
                "min_weight_fraction_leaf": "float, default=0.0",
                "description":"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range [0.0, 0.5]"
            },
            {
                "max_depth" : "int or None, default=3",
                "description": "Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range [1, inf)."
            },
            {
                "min_impurity_decrease": "float, default=0.0",
                "description": "A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n The weighted impurity decrease equation is the following: \n\n N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)"
            },
            {
                "init":"estimator or ‘zero’, default=None",
                 "description":"An estimator object that is used to compute the initial predictions. init has to provide fit and predict. If ‘zero’, the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss=’squared_error’), or a quantile for the other losses."

            },
            {
                "random_state":"int, RandomState instance or None, default=None",
                "description":"Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if n_iter_no_change is not None. Pass an int for reproducible output across multiple function calls. See Glossary."
            },
            {
                "max_features":"{‘sqrt’, ‘log2’}, int or float, default=None",
                "description":"The number of features to consider when looking for the best split:\n If int, values must be in the range [1, inf)\n If float, values must be in the range (0.0, 1.0] and the features considered at each split will be max(1, int(max_features * n_features_in_)) \n If “sqrt”, then max_features=sqrt(n_features). \n If “log2”, then max_features=log2(n_features).\n If None, then max_features=n_features."
            },
            {
                "alpha":"float, default=0.9",
                "description":"The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile'. Values must be in the range (0.0, 1.0)."
            },
            {
                "verbose":"int, default=0",
                "description":"Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range [0, inf)"
            },
            {
                "max_leaf_nodes":"int, default=None",
                "description":"Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range [2, inf). If None, then unlimited number of leaf nodes."
            },
            {
                "warm_start":"bool, default=False",
                "description":"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution."
            },
            {
                "validation_fraction":"float, default=0.1",
                "description":"The proportion of training data to set aside as validation set for early stopping. Values must be in the range (0.0, 1.0). Only used if n_iter_no_change is set to an integer."
            },
            {
                "n_iter_no_change":"int, default=None",
                "description":"n_iter_no_change is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside validation_fraction size of the training data as validation and terminate training when validation score is not improving in all of the previous n_iter_no_change numbers of iterations. Values must be in the range [1, inf). See Early stopping in Gradient Boosting."
            },
            {
                "tol":"float, default=1e-4",
                "description":"Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops. Values must be in the range [0.0, inf)."
            },
            {
                "ccp_alpha": "non-negative float, default=0.0",
                "description": "Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details."
            }
            

        ]
    }
}
